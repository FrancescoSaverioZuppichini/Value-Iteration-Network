{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Value Iteration Networks\n",
    "## Francesco Saverio Zuppichini\n",
    "\n",
    "*Authors: Aviv Tamar, Yi Wu, Garrett Thomas, Sergey Levine, and Pieter Abbeel*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Value Iteration Networks\n",
    "![title](https://raw.githubusercontent.com/FrancescoSaverioZuppichini/Value-Iteration-Network/master/resources/vin.png)\n",
    "\n",
    "Problem: *Planning* not able to generalise unseen domains \n",
    "\n",
    "Solution: A module able to **learn** to plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Markov Decision Process\n",
    "\n",
    "is a 5-tuple $(S, A, P, R, \\gamma)$ where:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "        \\begin{array}{l}\n",
    "         \\text{finite set of states:} \\\\\n",
    "                s \\in S \\\\\n",
    "         \\text{finite set of actions:} \\\\\n",
    "        a \\in A \\\\\n",
    "                 \\text{state transition probabilities:} \\\\\n",
    "        p(s' | s, a) = Pr \\{S_{t + 1} = s' | S_t = s, A_t = a \\} \\\\\n",
    "                \\text{expected reward for state-action-nexstate:}\\\\\n",
    "        r(s',s, a) = \\mathbb{E}[ R_{t + 1} | S_{t + 1} = s',  S_t = s, A_t = a ]  \\\\\n",
    "        \\end{array}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Reward\n",
    "\n",
    "$$G_t = \\sum_{k = 0}^H \\gamma^kr_{t + k + 1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Value Function\n",
    "\n",
    "*how good* is to be in a state\n",
    "\n",
    "$$\n",
    "V_{\\pi}(s) = \\mathbb{E}[G_t | S_t = s]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Bellman Equation\n",
    "\n",
    "**recursive** relation\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\begin{array}{l l}\n",
    "V_{\\pi}(s) & = \\mathbb{E}_{\\pi}\\begin{bmatrix}G_t | S_t = s\\end{bmatrix} \\\\\n",
    "\\\\\n",
    "& =  \\mathbb{E}_{\\pi}\\begin{bmatrix}\\sum\\limits_{k = 0}^{\\infty} \\gamma^kR_{t + k + 1} | S_t = s \\end{bmatrix} \\\\\n",
    "\\\\\n",
    "& = \\mathbb{E}_{\\pi}\\begin{bmatrix}R_{t + 1} + \\gamma \\sum\\limits_{k = 0}^{\\infty} \\gamma^k R_{t + k + 2} | S_t = s \\end{bmatrix} \\\\\n",
    "\\\\\n",
    "& = \\underbrace{\\sum\\limits_{a} \\pi(a | s) \\sum\\limits_{s'}\\sum\\limits_{r} p(s', r | s, a)}_{\\text{Sum of all probabilities $\\forall$ possible $r$}} \\\\\n",
    "& \\begin{bmatrix} r + \\gamma \\underbrace{\\mathbb{E}_{\\pi}\\begin{bmatrix} \\sum\\limits_{k = 0}^{\\infty} \\gamma^k R_{t + k + 2} | S_{t + 1} = s' \\end{bmatrix}}_{\\text{Expected reward from } s_{t + 1}} \\end{bmatrix} \\\\\n",
    "\\\\\n",
    "& = \\sum\\limits_{a} \\pi(a | s) \\sum\\limits_{s'}\\sum\\limits_{r} p(s', r | s, a)\\begin{bmatrix} r + \\gamma V_{\\pi}(s')\n",
    "\\end{bmatrix}\n",
    "\\end{array}\n",
    "\\label{eq: value_bellman}\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Thus we can **PLAN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Value Iteration\n",
    "\n",
    "![alt](https://raw.githubusercontent.com/FrancescoSaverioZuppichini/Value-Iteration-Network/master/resources/value_iteration.jpg)\n",
    "\n",
    "Does not *generalise*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we think about \n",
    "\n",
    "$$\n",
    "V_{n + 1}(s) = max_aQ_n(s,a) \\quad \\forall s\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "Q_n(s,a) = R(s,a) + \\gamma \\sum_{s'}P(s'|s,a)V_n(s')\n",
    "$$\n",
    "\n",
    "It is very similar to a **convolutional** and **max pooling** operation\n",
    "\n",
    "\n",
    "*paper notation*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we think about \n",
    "\n",
    "$$\n",
    "V_{n + 1}(s) = \\underbrace{max_a}_{\\text Max Pooling}Q_n(s,a) \\quad \\forall s\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "Q_n(s,a) = R(s,a) + \\underbrace{\\gamma \\sum_{s'}P(s'|s,a)}_{\\text{Convolution}}V_n(s')\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Convolution\n",
    "\n",
    "\n",
    "$$\n",
    "h_{l', i', j'} = \\sigma(\\sum_{l,i,j} W^{l'}_{l,i,j}X_{l, i'-i, j' - j})\n",
    "$$\n",
    "\n",
    "Max Pooling\n",
    "\n",
    "$$\n",
    "h_{l, i, j} ^{\\text{maxpool}} - max_{l', j' \\in N(i,j)} h_{l,i',j'}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thus we can represent Value Iteration with a **CNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Value Iteration Network\n",
    "\n",
    "![alt](https://raw.githubusercontent.com/FrancescoSaverioZuppichini/Value-Iteration-Network/master/resources/3.jpg)\n",
    "\n",
    "Assume there is a un uknowm $\\bar M$ and add the solution to $\\pi$ policy of $M$\n",
    "\n",
    "\n",
    "We want to learn $ \\bar R = f_r(\\phi(s)) $ and $ \\bar P = f_P(\\phi(s)) $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "R maps an image of the domain to a high reward at the goal and negative near an obstacle\n",
    "\n",
    "P encodes deterministic movement in the grid-woirld that do not depend on the observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Notice that\n",
    "\n",
    "$$ \\bar \\pi^* = argmax_{\\bar a} \\bar R (\\bar s, \\bar a) + \\gamma  \\sum_{\\bar s'} \\bar P (\\bar s'| \\bar s, \\bar a) \\bar V^*(\\bar s') $$\n",
    "\n",
    "**local connectivity** the states with $\\bar P (\\bar s'| \\bar s, \\bar a)$ small subset of $\\bar S$. **Attention**\n",
    "\n",
    "![alt](https://raw.githubusercontent.com/FrancescoSaverioZuppichini/Value-Iteration-Network/master/resources/attention.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Value Iteration Network\n",
    "![alt](https://raw.githubusercontent.com/FrancescoSaverioZuppichini/Value-Iteration-Network/master/resources/4.jpg)\n",
    "\n",
    "$$\n",
    "Q_{\\bar a, i', j'} = \\sum_{l, i, j} W^{\\bar a}_{l,i,j} \\bar R_{l, i' - i, j' - j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![alt](https://raw.githubusercontent.com/FrancescoSaverioZuppichini/Value-Iteration-Network/master/resources/1.jpg)\n",
    "\n",
    "new $V$ stacked ro $\\bar R$ and feed again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Grid World\n",
    "\n",
    "![alt](https://raw.githubusercontent.com/FrancescoSaverioZuppichini/Value-Iteration-Network/master/resources/6.jpg)\n",
    "\n",
    "\n",
    "![alt](https://raw.githubusercontent.com/FrancescoSaverioZuppichini/Value-Iteration-Network/master/resources/7.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mars Rover Navigation\n",
    "\n",
    "![alt](https://raw.githubusercontent.com/FrancescoSaverioZuppichini/Value-Iteration-Network/master/resources/12.jpg)\n",
    "\n",
    "\n",
    "VIN learn to plan *from natural image input* \n",
    "\n",
    "elevation/obstacles data not a part of the input\n",
    "\n",
    "$84.8%$ success rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Continuous Control\n",
    "![alt](https://raw.githubusercontent.com/FrancescoSaverioZuppichini/Value-Iteration-Network/master/resources/10.jpg)\n",
    "\n",
    "\n",
    "Continuous states and continuous actions\n",
    "\n",
    "Grid size 28x28\n",
    "\n",
    "VIN train on coarse grid representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "It is able to learn!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import GridWorldDataset\n",
    "from config import *\n",
    "from utils import *\n",
    "\n",
    "world = WORLD_28X28\n",
    "\n",
    "test_ds = GridWorldDataset(*world, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'core'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-dc2c2034c72f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'figure.figsize'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mvin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_model_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_random_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    540\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0mdeserialized_storage_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'core'"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "plt.rcParams['figure.figsize'] = (14,4)\n",
    "\n",
    "vin = torch.load(get_model_path(world))\n",
    "\n",
    "labels, s1, s2, obs = get_random_data(test_ds, device, idx=1)\n",
    "\n",
    "_, v, r_img = vin((s1, s2, obs), 20)\n",
    "\n",
    "fig = make_world_r_v_image(obs, v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "rise": {
   "enable_chalkboard": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
