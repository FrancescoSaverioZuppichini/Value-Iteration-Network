{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Value Iteration Networks\n",
    "## Francesco Saverio Zuppichini\n",
    "\n",
    "*Authors: Aviv Tamar, Yi Wu, Garrett Thomas, Sergey Levine, and Pieter Abbeel*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Value Iteration Networks\n",
    "![title](https://raw.githubusercontent.com/FrancescoSaverioZuppichini/Value-Iteration-Network/master/resources/vin.png)\n",
    "\n",
    "Problem: *Planning* not able to generalise unseen domains \n",
    "\n",
    "Solution: A module able to **learn** to plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Markov Decision Process\n",
    "\n",
    "is a 5-tuple $(S, A, P, R, \\gamma)$ where:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "        \\begin{array}{l}\n",
    "         \\text{finite set of states:} \\\\\n",
    "                s \\in S \\\\\n",
    "         \\text{finite set of actions:} \\\\\n",
    "        a \\in A \\\\\n",
    "                 \\text{state transition probabilities:} \\\\\n",
    "        p(s' | s, a) = Pr \\{S_{t + 1} = s' | S_t = s, A_t = a \\} \\\\\n",
    "                \\text{expected reward for state-action-nexstate:}\\\\\n",
    "        r(s',s, a) = \\mathbb{E}[ R_{t + 1} | S_{t + 1} = s',  S_t = s, A_t = a ]  \\\\\n",
    "        \\end{array}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Reward\n",
    "\n",
    "$$G_t = \\sum_{k = 0}^H \\gamma^kr_{t + k + 1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Value Function\n",
    "\n",
    "*how good* is to be in a state\n",
    "\n",
    "$$\n",
    "V_{\\pi}(s) = \\mathbb{E}[G_t | S_t = s]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Bellman Equation\n",
    "\n",
    "**recursive** relation\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\begin{array}{l l}\n",
    "V_{\\pi}(s) & = \\mathbb{E}_{\\pi}\\begin{bmatrix}G_t | S_t = s\\end{bmatrix} \\\\\n",
    "\\\\\n",
    "& =  \\mathbb{E}_{\\pi}\\begin{bmatrix}\\sum\\limits_{k = 0}^{\\infty} \\gamma^kR_{t + k + 1} | S_t = s \\end{bmatrix} \\\\\n",
    "\\\\\n",
    "& = \\mathbb{E}_{\\pi}\\begin{bmatrix}R_{t + 1} + \\gamma \\sum\\limits_{k = 0}^{\\infty} \\gamma^k R_{t + k + 2} | S_t = s \\end{bmatrix} \\\\\n",
    "\\\\\n",
    "& = \\underbrace{\\sum\\limits_{a} \\pi(a | s) \\sum\\limits_{s'}\\sum\\limits_{r} p(s', r | s, a)}_{\\text{Sum of all probabilities $\\forall$ possible $r$}} \\\\\n",
    "& \\begin{bmatrix} r + \\gamma \\underbrace{\\mathbb{E}_{\\pi}\\begin{bmatrix} \\sum\\limits_{k = 0}^{\\infty} \\gamma^k R_{t + k + 2} | S_{t + 1} = s' \\end{bmatrix}}_{\\text{Expected reward from } s_{t + 1}} \\end{bmatrix} \\\\\n",
    "\\\\\n",
    "& = \\sum\\limits_{a} \\pi(a | s) \\sum\\limits_{s'}\\sum\\limits_{r} p(s', r | s, a)\\begin{bmatrix} r + \\gamma V_{\\pi}(s')\n",
    "\\end{bmatrix}\n",
    "\\end{array}\n",
    "\\label{eq: value_bellman}\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Thus we can **PLAN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Value Iteration\n",
    "\n",
    "Find **optimal** policy by **planning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\n",
       "\\begin{algorithm}[H]\n",
       "  \\SetKwInOut{Output}{ouput}\n",
       " Initialise $V(s) \\in \\mathbb{R}, \\text{e.g} V(s) = 0$ \\\\\n",
       " $\\Delta \\leftarrow 0$ \\\\\n",
       " \\While{$\\Delta \\ge \\theta$ (a small positive number)}{\n",
       "  \\ForEach{$s \\in S$} {\n",
       "        $v \\leftarrow V(s)$\\\\\n",
       "        $V(s) \\leftarrow \\max\\limits_a \\sum\\limits_{s', r} p(s',r | s, a) \\begin{bmatrix}\n",
       "                r + \\gamma V(s')\n",
       "        \\end{bmatrix}$ \\\\\n",
       "        $\\Delta \\leftarrow \\max(\\Delta, | v - V(s)|)$\n",
       "  }\n",
       " }\n",
       " \\Output{Deterministic policy $\\pi \\approx \\pi_*$ such that}\n",
       " $\\pi(s) = \\argmax\\limits_a \\sum\\limits_{s', r} p(s',r | s, a) \\begin{bmatrix}\n",
       "     r + \\gamma V(s')\n",
       "  \\end{bmatrix}$\n",
       "\\caption{Value Iteration}\n",
       "\\end{algorithm}"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Latex\n",
    "\n",
    "Latex(r\"\"\"\n",
    "\\begin{algorithm}[H]\n",
    "  \\SetKwInOut{Output}{ouput}\n",
    " Initialise $V(s) \\in \\mathbb{R}, \\text{e.g} V(s) = 0$ \\\\\n",
    " $\\Delta \\leftarrow 0$ \\\\\n",
    " \\While{$\\Delta \\ge \\theta$ (a small positive number)}{\n",
    "  \\ForEach{$s \\in S$} {\n",
    "        $v \\leftarrow V(s)$\\\\\n",
    "        $V(s) \\leftarrow \\max\\limits_a \\sum\\limits_{s', r} p(s',r | s, a) \\begin{bmatrix}\n",
    "                r + \\gamma V(s')\n",
    "        \\end{bmatrix}$ \\\\\n",
    "        $\\Delta \\leftarrow \\max(\\Delta, | v - V(s)|)$\n",
    "  }\n",
    " }\n",
    " \\Output{Deterministic policy $\\pi \\approx \\pi_*$ such that}\n",
    " $\\pi(s) = \\argmax\\limits_a \\sum\\limits_{s', r} p(s',r | s, a) \\begin{bmatrix}\n",
    "     r + \\gamma V(s')\n",
    "  \\end{bmatrix}$\n",
    "\\caption{Value Iteration}\n",
    "\\end{algorithm}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
