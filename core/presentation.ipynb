{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Value Iteration Networks\n",
    "## Francesco Saverio Zuppichini\n",
    "\n",
    "*Authors: Aviv Tamar, Yi Wu, Garrett Thomas, Sergey Levine, and Pieter Abbeel*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Value Iteration Networks\n",
    "![title](https://raw.githubusercontent.com/FrancescoSaverioZuppichini/Value-Iteration-Network/master/resources/vin.png)\n",
    "\n",
    "Problem: *Planning* not able to generalise unseen domains \n",
    "\n",
    "Solution: A module able to **learn** to plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Markov Decision Process\n",
    "\n",
    "is a 5-tuple $(S, A, P, R, \\gamma)$ where:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "        \\begin{array}{l}\n",
    "         \\text{finite set of states:} \\\\\n",
    "                s \\in S \\\\\n",
    "         \\text{finite set of actions:} \\\\\n",
    "        a \\in A \\\\\n",
    "                 \\text{state transition probabilities:} \\\\\n",
    "        p(s' | s, a) = Pr \\{S_{t + 1} = s' | S_t = s, A_t = a \\} \\\\\n",
    "                \\text{expected reward for state-action-nexstate:}\\\\\n",
    "        r(s',s, a) = \\mathbb{E}[ R_{t + 1} | S_{t + 1} = s',  S_t = s, A_t = a ]  \\\\\n",
    "        \\end{array}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Reward\n",
    "\n",
    "$$G_t = \\sum_{k = 0}^H \\gamma^kr_{t + k + 1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Value Function\n",
    "\n",
    "*how good* is to be in a state\n",
    "\n",
    "$$\n",
    "V_{\\pi}(s) = \\mathbb{E}[G_t | S_t = s]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Bellman Equation\n",
    "\n",
    "**recursive** relation\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\begin{array}{l l}\n",
    "V_{\\pi}(s) & = \\mathbb{E}_{\\pi}\\begin{bmatrix}G_t | S_t = s\\end{bmatrix} \\\\\n",
    "\\\\\n",
    "& =  \\mathbb{E}_{\\pi}\\begin{bmatrix}\\sum\\limits_{k = 0}^{\\infty} \\gamma^kR_{t + k + 1} | S_t = s \\end{bmatrix} \\\\\n",
    "\\\\\n",
    "& = \\mathbb{E}_{\\pi}\\begin{bmatrix}R_{t + 1} + \\gamma \\sum\\limits_{k = 0}^{\\infty} \\gamma^k R_{t + k + 2} | S_t = s \\end{bmatrix} \\\\\n",
    "\\\\\n",
    "& = \\underbrace{\\sum\\limits_{a} \\pi(a | s) \\sum\\limits_{s'}\\sum\\limits_{r} p(s', r | s, a)}_{\\text{Sum of all probabilities $\\forall$ possible $r$}} \\\\\n",
    "& \\begin{bmatrix} r + \\gamma \\underbrace{\\mathbb{E}_{\\pi}\\begin{bmatrix} \\sum\\limits_{k = 0}^{\\infty} \\gamma^k R_{t + k + 2} | S_{t + 1} = s' \\end{bmatrix}}_{\\text{Expected reward from } s_{t + 1}} \\end{bmatrix} \\\\\n",
    "\\\\\n",
    "& = \\sum\\limits_{a} \\pi(a | s) \\sum\\limits_{s'}\\sum\\limits_{r} p(s', r | s, a)\\begin{bmatrix} r + \\gamma V_{\\pi}(s')\n",
    "\\end{bmatrix}\n",
    "\\end{array}\n",
    "\\label{eq: value_bellman}\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Thus we can **PLAN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Value Iteration\n",
    "\n",
    "![alt](https://raw.githubusercontent.com/FrancescoSaverioZuppichini/Value-Iteration-Network/master/resources/value_iteration.jpg)\n",
    "\n",
    "Does not *generalise*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we think about \n",
    "\n",
    "$$\n",
    "V_{n + 1}(s) = max_aQ_n(s,a) \\quad \\forall s\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "Q_n(s,a) = R(s,a) + \\gamma \\sum_{s'}P(s'|s,a)V_n(s')\n",
    "$$\n",
    "\n",
    "It is very similar to a **convolutional** and **max pooling** operation\n",
    "\n",
    "\n",
    "*paper notation*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we think about \n",
    "\n",
    "$$\n",
    "V_{n + 1}(s) = \\underbrace{max_a}_{\\text Max Pooling}Q_n(s,a) \\quad \\forall s\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "Q_n(s,a) = R(s,a) + \\underbrace{\\gamma \\sum_{s'}P(s'|s,a)}_{\\text{Convolution}}V_n(s')\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Convolution\n",
    "\n",
    "\n",
    "$$\n",
    "h_{l', i', j'} = \\sigma(\\sum_{l,i,j} W^{l'}_{l,i,j}X_{l, i'-i, j' - j})\n",
    "$$\n",
    "\n",
    "Max Pooling\n",
    "\n",
    "$$\n",
    "h_{l, i, j} ^{\\text{maxpool}} - max_{l', j' \\in N(i,j)} h_{l,i',j'}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thus we can represent Value Iteration with a **CNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Value Iteration Network\n",
    "\n",
    "![alt](https://raw.githubusercontent.com/FrancescoSaverioZuppichini/Value-Iteration-Network/master/resources/3.jpg)\n",
    "\n",
    "Assume there is a un uknowm $\\bar M$ and add the solution to $\\pi$ policy of $M$\n",
    "\n",
    "\n",
    "We want to learn $ \\bar R = f_r(\\phi(s)) $ and $ \\bar P = f_P(\\phi(s)) $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "R maps an image of the domain to a high reward at the goal and negative near an obstacle\n",
    "\n",
    "P encodes deterministic movement in the grid-woirld that do not depend on the observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Notice that\n",
    "\n",
    "$$ \\bar \\pi^* = argmax_{\\bar a} \\bar R (\\bar s, \\bar a) + \\gamma  \\sum_{\\bar s'} \\bar P (\\bar s'| \\bar s, \\bar a) \\bar V^*(\\bar s') $$\n",
    "\n",
    "**local connectivity** the states with $\\bar P (\\bar s'| \\bar s, \\bar a)$ small subset of $\\bar S$. **Attention**\n",
    "\n",
    "![alt](https://raw.githubusercontent.com/FrancescoSaverioZuppichini/Value-Iteration-Network/master/resources/11.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Value Iteration Network\n",
    "![alt](https://raw.githubusercontent.com/FrancescoSaverioZuppichini/Value-Iteration-Network/master/resources/4.jpg)\n",
    "\n",
    "$$\n",
    "Q_{\\bar a, i', j'} = \\sum_{l, i, j} W^{\\bar a}_{l,i,j} \\bar R_{l, i' - i, j' - j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![alt](https://raw.githubusercontent.com/FrancescoSaverioZuppichini/Value-Iteration-Network/master/resources/1.jpg)\n",
    "\n",
    "new $V$ stacked ro $\\bar R$ and feed again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Grid World\n",
    "\n",
    "![alt](https://raw.githubusercontent.com/FrancescoSaverioZuppichini/Value-Iteration-Network/master/resources/6.jpg)\n",
    "\n",
    "\n",
    "![alt](https://raw.githubusercontent.com/FrancescoSaverioZuppichini/Value-Iteration-Network/master/resources/7.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mars Rover Navigation\n",
    "\n",
    "![alt](https://raw.githubusercontent.com/FrancescoSaverioZuppichini/Value-Iteration-Network/master/resources/9.jpg)\n",
    "\n",
    "\n",
    "VIN learn to plan *from natural image input* \n",
    "\n",
    "elevation data not a part of the input\n",
    "\n",
    "$84.8%$ success rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Continuous Control\n",
    "![alt](https://raw.githubusercontent.com/FrancescoSaverioZuppichini/Value-Iteration-Network/master/resources/10.jpg)\n",
    "\n",
    "\n",
    "Continuous states and continuous actions\n",
    "\n",
    "VIN train on coarse grid representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "rise": {
   "enable_chalkboard": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
